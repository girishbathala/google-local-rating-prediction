{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# google-local-rating-prediction\n",
    "\n",
    "### 1. Objective\n",
    "The goal of this task is to predict a user’s star rating of a particular business. The following pipeline was\n",
    "followed to implement the task :\n",
    "\n",
    "### 2. Data-set preprocessing and preparation\n",
    "\n",
    "#### Note : Download and unzip the dataset from : http://jmcauley.ucsd.edu/data/assignment1.tar.gz\n",
    "\n",
    "The Google Local dataset has 200,000 reviews of businesses’ rated by users. The dataset contains many \n",
    "attributes but for the purpose of this task, only the following were used:\n",
    "* businessID: The ID of the business. This is a hashed product identifier from Google.\n",
    "* userID: The ID of the reviewer. This is a hashed user identifier from Google.\n",
    "* rating: The star rating of the user’s review.\n",
    "\n",
    "The dataset is divided into train, validation using k-fold ( k = 3) split. The entire 200,000 data points were used for building\n",
    "the final model once all the parameters were estimated after validation\n",
    "\n",
    "### 3. Recommender System Models Used\n",
    "\n",
    "The approach used was to build an ensemble of 6 different recommender system algorithms to model the\n",
    "relation between the users and businesses. For each algorithm the 200,000 X 3 train-set was converted into a \n",
    "pandas dataframe and then was represented as a surprise trainset object before passing it into surprise’s algorithm libraries\n",
    "\n",
    "### 3.1 BaselineOnly\n",
    "Algorithm predicting the baseline rating estimate $\\hat{r}_{ui}$ for given user $u$ and item $i$.\n",
    "\n",
    "$$\\hat{r}_{ui}=b_{ui}= \\mu +b_u+ b_i $$\n",
    "\n",
    "If user $u$ is unknown, then the bias $b_u$ is assumed to be zero. The same applies for item $i$ with $b_i.$\n",
    "\n",
    "This model tries to minimize the following regularized  square error, where $\\lambda$ is the regularization parameter.\n",
    "\n",
    "$$\\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - (\\mu + b_u + b_i)\\right)^2 +\n",
    "\\lambda \\left(b_u^2 + b_i^2 \\right)$$\n",
    "\n",
    "\n",
    "Two separate models are built using the \"baselineOnly\" idea from surprise :\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)**: The parameters for SGD are :\n",
    "\n",
    "* ’reg’ or lambda: The regularization parameter of the cost function that is optimized\n",
    "* ’learning rate’: The learning rate of SGD\n",
    "* ’nepochs’: The number of iteration of the SGD procedure\n",
    "\n",
    "**Alternating Least Squares (ALS) **: The update equation for $\\mu$ is the simple average of all the all ratings centered after subtracting $b_{u}$ + $b_{i}$. The update equations for $b_u$ and $b_i$ are in the figure below, where K is the $R_{trainset}$, $\\lambda_2$ = $reg_i$, and $\\lambda_3$ = $reg_u,$\n",
    "\n",
    "![](images-readme/als.png)\n",
    "\n",
    "* ’regi’: The regularization parameter for items.\n",
    "* ’regu’: The regularization parameter for users.\n",
    "* ’nepochs’: The number of iteration of the ALS procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 KNNBaseline\n",
    "\n",
    "A basic collaborative filtering algorithm taking into account a baseline rating. It also accounts for the similarity sim(u,v) between user pairs (u,v). Let $N^k_i(u)$ denote the K-user-neighbourhood of user u and people who have rated item $i$. The predicted rating $r_{ui}$ is given by :\n",
    "\n",
    "\n",
    "$$\\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in\n",
    "N^k_i(u)} \\text{sim}(u, v)}$$\n",
    "\n",
    "The similarity between users is computed by using the pearson similarity between user u and v $\\in$ $N^k_i(u)$ for parameter K and item i. These sim(u,v) values are i,j positions in similarity Matrix R for the ratings/ utility matrix C, as in figure below\n",
    "\n",
    "![](images-readme/pearson.png)\n",
    "\n",
    "Parameters for the KNN model are:\n",
    "\n",
    "* K – The (max) number of neighbors to take into account for aggregation\n",
    "- simoptions – Fixed for this task, pearson similarity\n",
    "+ bsloptions – Similar to baseline updates and parameters in previous section \"BaselineOnly\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Latent Factor Model : SVD \n",
    "The utility matrix U is approximated in a lower dimension form as two matrices Q and P. Each representing the low dimensional representation of a user and business. The low dim representation is K = 1 for this task. \n",
    "\n",
    "The prediction $\\hat{r}_i$ is set as:\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^Tp_u$$\n",
    "\n",
    "If user $u$ is unknown, then the bias $b_u$ and the factors $p_u$ are assumed to be zero. The same applies for item ii with $b_i$ and $q_i$.\n",
    "\n",
    "To estimate all the unknown, we need to minimize the following regularized squared error:\n",
    "$$\n",
    "\\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\right)\n",
    "$$\n",
    "\n",
    "The minimization is performed by a very straightforward stochastic gradient descent:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}b_u &\\leftarrow b_u + \\gamma (e_{ui} - \\lambda b_u)\\\\\n",
    "b_i &\\leftarrow b_i + \\gamma (e_{ui} - \\lambda b_i)\\\\\n",
    "p_u &\\leftarrow p_u + \\gamma (e_{ui} \\cdot q_i - \\lambda p_u)\\\\\n",
    "q_i &\\leftarrow q_i + \\gamma (e_{ui} \\cdot p_u - \\lambda q_i)\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where $e_{ui}=r_{ui}$-$\\hat{r}_{ui}$ \n",
    "These steps are performed over all the ratings of the trainset and repeated $n_{epochs}$ times. Baselines are initialized to $0$. User and item factors are randomly initialized according to a normal distribution\n",
    "\n",
    "The parameters for SVD are as follows :\n",
    "\n",
    "* nfactors – The number of factors.\n",
    "* nepochs – The number of iteration of the SGD procedure\n",
    "* lrall – The learning rate for all parameters.\n",
    "* regall – The regularization term for all parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Non-Negative Matrix Factorization : NMF \n",
    "A collaborative filtering algorithm based on Non-negative Matrix Factorization. This algorithm is very similar to SVD. The prediction $\\hat{r}_ui$ is set as:\n",
    "\n",
    "$$\\hat{r}_{ui} = q_i^Tp_u,$$\n",
    "\n",
    "where user and item factors are kept positive. \n",
    "The optimization procedure is a (regularized) stochastic gradient descent with a specific choice of step size that ensures non-negativity of factors, provided that their initial values are also positive.\n",
    "\n",
    "At each step of the SGD procedure, the factors f or user u and item i are updated as follows:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}p_{uf} &\\leftarrow p_{uf} \\cdot \\frac{\\sum_{i \\in I_u} q_{if}\n",
    "\\cdot r_{ui}}{\\sum_{i \\in I_u} q_{if} \\cdot \\hat{r_{ui}} +\n",
    "\\lambda_u |I_u| p_{uf}}\\\\\n",
    "q_{if} &\\leftarrow q_{if} \\cdot \\frac{\\sum_{u \\in U_i} p_{uf}\n",
    "\\cdot r_{ui}}{\\sum_{u \\in U_i} p_{uf} \\cdot \\hat{r_{ui}} +\n",
    "\\lambda_i |U_i| q_{if}}\\\\\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda_u$ and $\\lambda_i$ are regularization parameters\n",
    "The Parameters for the model are :\n",
    "\n",
    "\n",
    "* n_factors – The number of factors.\n",
    "* n_epochs – The number of iteration of the SGD procedure.\n",
    "* reg_pu – The regularization term for users λu.\n",
    "* reg_qi – The regularization term for items λi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Slope One:\n",
    "A simple yet accurate collaborative filtering algorithm.The prediction $\\hat{r}_{ui}$ is set as:\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu_u + \\dfrac{1}{\n",
    "|R_i(u)|}\n",
    "\\sum\\limits_{j \\in R_i(u)} \\text{dev}(i, j)$$\n",
    "\n",
    "\n",
    "where $R_i(u)$ is the set of relevant items, i.e. the set of items $j$ rated by $u$ that also have at least one common user with $i$. $Dev(i,j)$ is defined as the average difference between the ratings of $i$ and those of $j$:\n",
    "\n",
    "$$\n",
    "\\text{dev}(i, j) = \\dfrac{1}{\n",
    "|U_{ij}|}\\sum\\limits_{u \\in U_{ij}} r_{ui} - r_{uj}$$\n",
    "\n",
    "\n",
    "### 3.6 Prediction and Validation\n",
    "\n",
    "The 6 models presented above are validated independently on the 100,000 validation set to find their respective best parameters. The Baseline ALS algorithm, Latent Factor Model, and KNNBaseline were found to be the best performing out of the three. The predictions were made based on the following update equation\n",
    "\n",
    "$$ \\hat{r}_{ui} = \\dfrac{\\sum_{k = 1}^{k = 6}w_k * \\hat{r}_{uik}}{\\sum_{k=1}^{k=6} w_k}$$\n",
    "\n",
    "where, $w_k$ are the weights associated with each model prediction and $\\hat{r}_{uik}$ is the predicted rating of each model. This ensemble model was validated to get the best performing weights $w_k$. The final model was trained using the best parameters and the entire training set (200,000). The weighted average of these models outputs  on the test set were submitted on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
